{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hand Gesture Recogination Convolutional Neural Network\n",
    "  \n",
    "Author:       Mansur Amin <br>\n",
    "Team:         ASL Dynamics <br>\n",
    "Team Members: Matt Wnuk, Juan Castillo <br>\n",
    "Class:        ECE 285 Spring 2018 : Machine Learning for Computer Vision <br>\n",
    "\n",
    "- Build a convolutional neural network with TensorFlow for for gesture recogination using the MNIST ASL Dataset avaiable on Kaggel.\n",
    "\n",
    "- This project uses TensorFlow layers API for a raw TensorFlow implementation with variables. <br>\n",
    "Reference: Aymeric Damien's project to recognize digits in images from the MNIST digits dataset <br>\n",
    "Reference source: https://github.com/aymericdamien/TensorFlow-Examples/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Overview\n",
    "\n",
    "Convolutional Neural network \n",
    "\n",
    "![CNN](https://cdn-images-1.medium.com/max/1400/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST ASL Hand Gestures. The dataset contains 27,456 examples for training and 7,173 examples for testing. The images have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST](https://www.kaggle.com/databundle/preview/image-dataset/3258/5337/5337/15172)\n",
    "\n",
    "More info: https://www.kaggle.com/datamunge/sign-language-mnist/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# Libraries \n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "#####################################################################\n",
    "# def read_data(path) :\n",
    "# Reads data specidfed by path and stores into numpy array, Splits data nad lbl\n",
    "# path = location of where data is stored\n",
    "#####################################################################\n",
    "def read_data(path) :\n",
    "    num_lines = sum(1 for line in open(path)) # get number of lines in file\n",
    "    myfile = open(path,'r')\n",
    "    j = 0\n",
    "    read_lines = np.zeros((num_lines,785))    # expects a 25x25 pixel row vector\n",
    "    for line in myfile :\n",
    "        if(j >= 1) :\n",
    "            read_lines[j,:] = np.asarray(line.split(','))\n",
    "        j = j+1\n",
    "    myfile.close()\n",
    "    \n",
    "    print('Initial', path[63:68], 'Shape:', read_lines.shape)\n",
    "    read_lines = np.delete(read_lines,0,0) # remove pixel labels row from frost row\n",
    "    labels = read_lines[:,0]\n",
    "    read_lines = np.delete(read_lines,0,1) # remove labels from first collumn\n",
    "    print('Augmented', path[63:68], 'Shape:', read_lines.shape)\n",
    "    return read_lines,labels.astype(np.int16) # .reshape(len(labels),1)\n",
    "\n",
    "#####################################################################\n",
    "# def vis_data(dat_vec,dat_lbl) :\n",
    "# Visualize data.\n",
    "# dat_vec = vector representing data\n",
    "# dat_lbl = label representing data\n",
    "#####################################################################\n",
    "def vis_data(dat_vec,dat_lbl) :\n",
    "    lbl_map = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "    dat_mat = dat_vec.reshape(28,28)\n",
    "    plt.title('Image Label = ' + str(dat_lbl) + ' = ' + lbl_map[dat_lbl])\n",
    "    plt.imshow(dat_mat)\n",
    "    \n",
    "#####################################################################\n",
    "# def normalize_MNIST_images(x) :\n",
    "# Normalize data in range of -1 to 1\n",
    "# x = vector representing data\n",
    "#####################################################################\n",
    "def normalize_MNIST_images(x,max_x, diff) : \n",
    "    x = x.astype(np.float64)\n",
    "    x = max_x*(x-np.min(x))/(np.max(x)-np.min(x))-diff\n",
    "    print(\"Normalize_MNIST_images\", x.shape)\n",
    "    return x\n",
    "\n",
    "def onehot2label(d) : \n",
    "    lbl = 9-d.argmax(axis=0)\n",
    "    lbl = lbl.reshape(1,np.size(lbl))\n",
    "    print('onehot2label',lbl.shape)\n",
    "    return lbl\n",
    "\n",
    "def label2onehot(lbl) : \n",
    "    d = np.zeros((lbl.max() + 1, lbl.size))\n",
    "    d[lbl.max()-lbl, np.arange(0,lbl.size)] = 1\n",
    "    print('label2onehot', d.shape)\n",
    "    return d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train Shape: (27456, 785)\n",
      "Augmented train Shape: (27455, 784)\n",
      "Initial test. Shape: (7173, 785)\n",
      "Augmented test. Shape: (7172, 784)\n",
      "Normalize_MNIST_images (27455, 784)\n",
      "Normalize_MNIST_images (7172, 784)\n",
      "label2onehot (25, 27455)\n",
      "label2onehot (25, 7172)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read ASL MNIST data\n",
    "####################################################################\n",
    "# # Windows Path\n",
    "# train_path = 'C:\\\\Users\\\\maamin\\\\Desktop\\\\ECE_285_ML_DL\\\\data\\\\sign_mnist_train.csv'\n",
    "# test_path = 'C:\\\\Users\\\\maamin\\\\Desktop\\\\ECE_285_ML_DL\\\\data\\\\sign_mnist_test.csv'\n",
    "####################################################################\n",
    "# # Mac path\n",
    "# train_path = '/Users/mansuramin/Desktop/ECE_285_ML/data/sign_mnist_train.csv'\n",
    "# test_path = '/Users/mansuramin/Desktop/ECE_285_ML/data/sign_mnist_test.csv'\n",
    "####################################################################\n",
    "# Linux Server Path\n",
    "train_path = '/datasets/ee285s-public/KaggleASL/Kaggle_ASL_Images/sign_mnist_train.csv'\n",
    "test_path = '/datasets/ee285s-public/KaggleASL/Kaggle_ASL_Images/sign_mnist_test.csv'\n",
    "####################################################################\n",
    "# Load data from path\n",
    "xtrain,ltrain = read_data(train_path)\n",
    "xtest,ltest = read_data(test_path)\n",
    "####################################################################\n",
    "# Normalize between 0 and 1\n",
    "xtrain = normalize_MNIST_images(xtrain,1,0)\n",
    "xtest = normalize_MNIST_images(xtest,1,0)\n",
    "####################################################################\n",
    "# Change Types to match nn types\n",
    "xtrain = xtrain.astype(np.float32)\n",
    "ltrain = ltrain.astype(np.uint8)\n",
    "xtest = xtest.astype(np.float32)\n",
    "ltest = ltest.astype(np.uint8)\n",
    "####################################################################\n",
    "# Visualize an image as test\n",
    "indx = 10240\n",
    "vis_data(xtrain[indx,:],ltrain[indx])\n",
    "####################################################################\n",
    "# convert to one hot\n",
    "ltrain = label2onehot(ltrain)\n",
    "ltest = label2onehot(ltest)\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 97205.8281, Training Accuracy= 0.070\n",
      "Step 10, Minibatch Loss= 31036.7891, Training Accuracy= 0.172\n",
      "Step 20, Minibatch Loss= 14853.6748, Training Accuracy= 0.406\n",
      "Step 30, Minibatch Loss= 6860.9937, Training Accuracy= 0.750\n",
      "Step 40, Minibatch Loss= 5116.1279, Training Accuracy= 0.703\n",
      "Step 50, Minibatch Loss= 3708.5796, Training Accuracy= 0.797\n",
      "Step 60, Minibatch Loss= 4308.7461, Training Accuracy= 0.781\n",
      "Step 70, Minibatch Loss= 5059.3652, Training Accuracy= 0.820\n",
      "Step 80, Minibatch Loss= 3625.9707, Training Accuracy= 0.812\n",
      "Step 90, Minibatch Loss= 1658.4265, Training Accuracy= 0.898\n",
      "Step 100, Minibatch Loss= 2841.1470, Training Accuracy= 0.844\n",
      "Step 110, Minibatch Loss= 1812.2323, Training Accuracy= 0.867\n",
      "Step 120, Minibatch Loss= 1681.3087, Training Accuracy= 0.906\n",
      "Step 130, Minibatch Loss= 1796.3453, Training Accuracy= 0.914\n",
      "Step 140, Minibatch Loss= 1400.1082, Training Accuracy= 0.914\n",
      "Step 150, Minibatch Loss= 1975.0845, Training Accuracy= 0.875\n",
      "Step 160, Minibatch Loss= 1639.6948, Training Accuracy= 0.906\n",
      "Step 170, Minibatch Loss= 1331.2532, Training Accuracy= 0.906\n",
      "Step 180, Minibatch Loss= 1759.0059, Training Accuracy= 0.898\n",
      "Step 190, Minibatch Loss= 1387.8447, Training Accuracy= 0.938\n",
      "Step 200, Minibatch Loss= 1865.8685, Training Accuracy= 0.883\n",
      "Step 210, Minibatch Loss= 1265.0752, Training Accuracy= 0.938\n",
      "Step 220, Minibatch Loss= 987.2871, Training Accuracy= 0.922\n",
      "Step 230, Minibatch Loss= 2177.5615, Training Accuracy= 0.914\n",
      "Step 240, Minibatch Loss= 1307.0850, Training Accuracy= 0.945\n",
      "Step 250, Minibatch Loss= 1505.0437, Training Accuracy= 0.898\n",
      "Step 260, Minibatch Loss= 746.3088, Training Accuracy= 0.945\n",
      "Step 270, Minibatch Loss= 1569.5448, Training Accuracy= 0.930\n",
      "Step 280, Minibatch Loss= 1037.5422, Training Accuracy= 0.930\n",
      "Step 290, Minibatch Loss= 786.7745, Training Accuracy= 0.930\n",
      "Step 300, Minibatch Loss= 1090.0668, Training Accuracy= 0.922\n",
      "Step 310, Minibatch Loss= 1327.4692, Training Accuracy= 0.906\n",
      "Step 320, Minibatch Loss= 1257.5536, Training Accuracy= 0.930\n",
      "Step 330, Minibatch Loss= 1247.8492, Training Accuracy= 0.945\n",
      "Step 340, Minibatch Loss= 1279.8809, Training Accuracy= 0.938\n",
      "Step 350, Minibatch Loss= 680.7175, Training Accuracy= 0.930\n",
      "Step 360, Minibatch Loss= 1626.3099, Training Accuracy= 0.930\n",
      "Step 370, Minibatch Loss= 654.2693, Training Accuracy= 0.922\n",
      "Step 380, Minibatch Loss= 802.2275, Training Accuracy= 0.961\n",
      "Step 390, Minibatch Loss= 759.4382, Training Accuracy= 0.945\n",
      "Step 400, Minibatch Loss= 476.7026, Training Accuracy= 0.945\n",
      "Step 410, Minibatch Loss= 843.7367, Training Accuracy= 0.930\n",
      "Step 420, Minibatch Loss= 756.6776, Training Accuracy= 0.961\n",
      "Step 430, Minibatch Loss= 22.5998, Training Accuracy= 0.984\n",
      "Step 440, Minibatch Loss= 1249.0638, Training Accuracy= 0.938\n",
      "Step 450, Minibatch Loss= 374.2090, Training Accuracy= 0.961\n",
      "Step 460, Minibatch Loss= 314.7913, Training Accuracy= 0.969\n",
      "Step 470, Minibatch Loss= 836.3682, Training Accuracy= 0.938\n",
      "Step 480, Minibatch Loss= 180.5121, Training Accuracy= 0.984\n",
      "Step 490, Minibatch Loss= 911.7241, Training Accuracy= 0.938\n",
      "Step 500, Minibatch Loss= 388.6938, Training Accuracy= 0.945\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9765625\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "#####################################################################\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "#####################################################################\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(NB).astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27455"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "N = xtrain.shape[0]            # training set size \n",
    "B = batch_size                  # minibatch size\n",
    "NB = N / B                      # number of minibatches\n",
    "NB = np.round(NB).astype(np.long)\n",
    "\n",
    "num_steps = NB #500\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 25 # MNIST total classes (0-25 gestures)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 1 batchx_y = (128, 784) (128, 10)\n",
      "Step 1, Minibatch Loss= 58034.8555, Training Accuracy= 0.031\n",
      "step= 2 batchx_y = (128, 784) (128, 10)\n",
      "step= 3 batchx_y = (128, 784) (128, 10)\n",
      "step= 4 batchx_y = (128, 784) (128, 10)\n",
      "step= 5 batchx_y = (128, 784) (128, 10)\n",
      "step= 6 batchx_y = (128, 784) (128, 10)\n",
      "step= 7 batchx_y = (128, 784) (128, 10)\n",
      "step= 8 batchx_y = (128, 784) (128, 10)\n",
      "step= 9 batchx_y = (128, 784) (128, 10)\n",
      "step= 10 batchx_y = (128, 784) (128, 10)\n",
      "Step 10, Minibatch Loss= 28193.5547, Training Accuracy= 0.047\n",
      "step= 11 batchx_y = (128, 784) (128, 10)\n",
      "step= 12 batchx_y = (128, 784) (128, 10)\n",
      "step= 13 batchx_y = (128, 784) (128, 10)\n",
      "step= 14 batchx_y = (128, 784) (128, 10)\n",
      "step= 15 batchx_y = (128, 784) (128, 10)\n",
      "step= 16 batchx_y = (128, 784) (128, 10)\n",
      "step= 17 batchx_y = (128, 784) (128, 10)\n",
      "step= 18 batchx_y = (128, 784) (128, 10)\n",
      "step= 19 batchx_y = (128, 784) (128, 10)\n",
      "step= 20 batchx_y = (128, 784) (128, 10)\n",
      "Step 20, Minibatch Loss= 17101.3555, Training Accuracy= 0.031\n",
      "step= 21 batchx_y = (128, 784) (128, 10)\n",
      "step= 22 batchx_y = (128, 784) (128, 10)\n",
      "step= 23 batchx_y = (128, 784) (128, 10)\n",
      "step= 24 batchx_y = (128, 784) (128, 10)\n",
      "step= 25 batchx_y = (128, 784) (128, 10)\n",
      "step= 26 batchx_y = (128, 784) (128, 10)\n",
      "step= 27 batchx_y = (128, 784) (128, 10)\n",
      "step= 28 batchx_y = (128, 784) (128, 10)\n",
      "step= 29 batchx_y = (128, 784) (128, 10)\n",
      "step= 30 batchx_y = (128, 784) (128, 10)\n",
      "Step 30, Minibatch Loss= 11948.4141, Training Accuracy= 0.117\n",
      "step= 31 batchx_y = (128, 784) (128, 10)\n",
      "step= 32 batchx_y = (128, 784) (128, 10)\n",
      "step= 33 batchx_y = (128, 784) (128, 10)\n",
      "step= 34 batchx_y = (128, 784) (128, 10)\n",
      "step= 35 batchx_y = (128, 784) (128, 10)\n",
      "step= 36 batchx_y = (128, 784) (128, 10)\n",
      "step= 37 batchx_y = (128, 784) (128, 10)\n",
      "step= 38 batchx_y = (128, 784) (128, 10)\n",
      "step= 39 batchx_y = (128, 784) (128, 10)\n",
      "step= 40 batchx_y = (128, 784) (128, 10)\n",
      "Step 40, Minibatch Loss= 10846.3086, Training Accuracy= 0.133\n",
      "step= 41 batchx_y = (128, 784) (128, 10)\n",
      "step= 42 batchx_y = (128, 784) (128, 10)\n",
      "step= 43 batchx_y = (128, 784) (128, 10)\n",
      "step= 44 batchx_y = (128, 784) (128, 10)\n",
      "step= 45 batchx_y = (128, 784) (128, 10)\n",
      "step= 46 batchx_y = (128, 784) (128, 10)\n",
      "step= 47 batchx_y = (128, 784) (128, 10)\n",
      "step= 48 batchx_y = (128, 784) (128, 10)\n",
      "step= 49 batchx_y = (128, 784) (128, 10)\n",
      "step= 50 batchx_y = (128, 784) (128, 10)\n",
      "Step 50, Minibatch Loss= 8021.5400, Training Accuracy= 0.188\n",
      "step= 51 batchx_y = (128, 784) (128, 10)\n",
      "step= 52 batchx_y = (128, 784) (128, 10)\n",
      "step= 53 batchx_y = (128, 784) (128, 10)\n",
      "step= 54 batchx_y = (128, 784) (128, 10)\n",
      "step= 55 batchx_y = (128, 784) (128, 10)\n",
      "step= 56 batchx_y = (128, 784) (128, 10)\n",
      "step= 57 batchx_y = (128, 784) (128, 10)\n",
      "step= 58 batchx_y = (128, 784) (128, 10)\n",
      "step= 59 batchx_y = (128, 784) (128, 10)\n",
      "step= 60 batchx_y = (128, 784) (128, 10)\n",
      "Step 60, Minibatch Loss= 6290.4307, Training Accuracy= 0.242\n",
      "step= 61 batchx_y = (128, 784) (128, 10)\n",
      "step= 62 batchx_y = (128, 784) (128, 10)\n",
      "step= 63 batchx_y = (128, 784) (128, 10)\n",
      "step= 64 batchx_y = (128, 784) (128, 10)\n",
      "step= 65 batchx_y = (128, 784) (128, 10)\n",
      "step= 66 batchx_y = (128, 784) (128, 10)\n",
      "step= 67 batchx_y = (128, 784) (128, 10)\n",
      "step= 68 batchx_y = (128, 784) (128, 10)\n",
      "step= 69 batchx_y = (128, 784) (128, 10)\n",
      "step= 70 batchx_y = (128, 784) (128, 10)\n",
      "Step 70, Minibatch Loss= 6333.5347, Training Accuracy= 0.297\n",
      "step= 71 batchx_y = (128, 784) (128, 10)\n",
      "step= 72 batchx_y = (128, 784) (128, 10)\n",
      "step= 73 batchx_y = (128, 784) (128, 10)\n",
      "step= 74 batchx_y = (128, 784) (128, 10)\n",
      "step= 75 batchx_y = (128, 784) (128, 10)\n",
      "step= 76 batchx_y = (128, 784) (128, 10)\n",
      "step= 77 batchx_y = (128, 784) (128, 10)\n",
      "step= 78 batchx_y = (128, 784) (128, 10)\n",
      "step= 79 batchx_y = (128, 784) (128, 10)\n",
      "step= 80 batchx_y = (128, 784) (128, 10)\n",
      "Step 80, Minibatch Loss= 4189.1172, Training Accuracy= 0.375\n",
      "step= 81 batchx_y = (128, 784) (128, 10)\n",
      "step= 82 batchx_y = (128, 784) (128, 10)\n",
      "step= 83 batchx_y = (128, 784) (128, 10)\n",
      "step= 84 batchx_y = (128, 784) (128, 10)\n",
      "step= 85 batchx_y = (128, 784) (128, 10)\n",
      "step= 86 batchx_y = (128, 784) (128, 10)\n",
      "step= 87 batchx_y = (128, 784) (128, 10)\n",
      "step= 88 batchx_y = (128, 784) (128, 10)\n",
      "step= 89 batchx_y = (128, 784) (128, 10)\n",
      "step= 90 batchx_y = (128, 784) (128, 10)\n",
      "Step 90, Minibatch Loss= 4745.9136, Training Accuracy= 0.359\n",
      "step= 91 batchx_y = (128, 784) (128, 10)\n",
      "step= 92 batchx_y = (128, 784) (128, 10)\n",
      "step= 93 batchx_y = (128, 784) (128, 10)\n",
      "step= 94 batchx_y = (128, 784) (128, 10)\n",
      "step= 95 batchx_y = (128, 784) (128, 10)\n",
      "step= 96 batchx_y = (128, 784) (128, 10)\n",
      "step= 97 batchx_y = (128, 784) (128, 10)\n",
      "step= 98 batchx_y = (128, 784) (128, 10)\n",
      "step= 99 batchx_y = (128, 784) (128, 10)\n",
      "step= 100 batchx_y = (128, 784) (128, 10)\n",
      "Step 100, Minibatch Loss= 4082.6765, Training Accuracy= 0.391\n",
      "step= 101 batchx_y = (128, 784) (128, 10)\n",
      "step= 102 batchx_y = (128, 784) (128, 10)\n",
      "step= 103 batchx_y = (128, 784) (128, 10)\n",
      "step= 104 batchx_y = (128, 784) (128, 10)\n",
      "step= 105 batchx_y = (128, 784) (128, 10)\n",
      "step= 106 batchx_y = (128, 784) (128, 10)\n",
      "step= 107 batchx_y = (128, 784) (128, 10)\n",
      "step= 108 batchx_y = (128, 784) (128, 10)\n",
      "step= 109 batchx_y = (128, 784) (128, 10)\n",
      "step= 110 batchx_y = (128, 784) (128, 10)\n",
      "Step 110, Minibatch Loss= 3601.0088, Training Accuracy= 0.375\n",
      "step= 111 batchx_y = (128, 784) (128, 10)\n",
      "step= 112 batchx_y = (128, 784) (128, 10)\n",
      "step= 113 batchx_y = (128, 784) (128, 10)\n",
      "step= 114 batchx_y = (128, 784) (128, 10)\n",
      "step= 115 batchx_y = (128, 784) (128, 10)\n",
      "step= 116 batchx_y = (128, 784) (128, 10)\n",
      "step= 117 batchx_y = (128, 784) (128, 10)\n",
      "step= 118 batchx_y = (128, 784) (128, 10)\n",
      "step= 119 batchx_y = (128, 784) (128, 10)\n",
      "step= 120 batchx_y = (128, 784) (128, 10)\n",
      "Step 120, Minibatch Loss= 2674.6455, Training Accuracy= 0.445\n",
      "step= 121 batchx_y = (128, 784) (128, 10)\n",
      "step= 122 batchx_y = (128, 784) (128, 10)\n",
      "step= 123 batchx_y = (128, 784) (128, 10)\n",
      "step= 124 batchx_y = (128, 784) (128, 10)\n",
      "step= 125 batchx_y = (128, 784) (128, 10)\n",
      "step= 126 batchx_y = (128, 784) (128, 10)\n",
      "step= 127 batchx_y = (128, 784) (128, 10)\n",
      "step= 128 batchx_y = (128, 784) (128, 10)\n",
      "step= 129 batchx_y = (128, 784) (128, 10)\n",
      "step= 130 batchx_y = (128, 784) (128, 10)\n",
      "Step 130, Minibatch Loss= 2867.2832, Training Accuracy= 0.414\n",
      "step= 131 batchx_y = (128, 784) (128, 10)\n",
      "step= 132 batchx_y = (128, 784) (128, 10)\n",
      "step= 133 batchx_y = (128, 784) (128, 10)\n",
      "step= 134 batchx_y = (128, 784) (128, 10)\n",
      "step= 135 batchx_y = (128, 784) (128, 10)\n",
      "step= 136 batchx_y = (128, 784) (128, 10)\n",
      "step= 137 batchx_y = (128, 784) (128, 10)\n",
      "step= 138 batchx_y = (128, 784) (128, 10)\n",
      "step= 139 batchx_y = (128, 784) (128, 10)\n",
      "step= 140 batchx_y = (128, 784) (128, 10)\n",
      "Step 140, Minibatch Loss= 1904.4758, Training Accuracy= 0.469\n",
      "step= 141 batchx_y = (128, 784) (128, 10)\n",
      "step= 142 batchx_y = (128, 784) (128, 10)\n",
      "step= 143 batchx_y = (128, 784) (128, 10)\n",
      "step= 144 batchx_y = (128, 784) (128, 10)\n",
      "step= 145 batchx_y = (128, 784) (128, 10)\n",
      "step= 146 batchx_y = (128, 784) (128, 10)\n",
      "step= 147 batchx_y = (128, 784) (128, 10)\n",
      "step= 148 batchx_y = (128, 784) (128, 10)\n",
      "step= 149 batchx_y = (128, 784) (128, 10)\n",
      "step= 150 batchx_y = (128, 784) (128, 10)\n",
      "Step 150, Minibatch Loss= 1947.4507, Training Accuracy= 0.570\n",
      "step= 151 batchx_y = (128, 784) (128, 10)\n",
      "step= 152 batchx_y = (128, 784) (128, 10)\n",
      "step= 153 batchx_y = (128, 784) (128, 10)\n",
      "step= 154 batchx_y = (128, 784) (128, 10)\n",
      "step= 155 batchx_y = (128, 784) (128, 10)\n",
      "step= 156 batchx_y = (128, 784) (128, 10)\n",
      "step= 157 batchx_y = (128, 784) (128, 10)\n",
      "step= 158 batchx_y = (128, 784) (128, 10)\n",
      "step= 159 batchx_y = (128, 784) (128, 10)\n",
      "step= 160 batchx_y = (128, 784) (128, 10)\n",
      "Step 160, Minibatch Loss= 2025.7805, Training Accuracy= 0.500\n",
      "step= 161 batchx_y = (128, 784) (128, 10)\n",
      "step= 162 batchx_y = (128, 784) (128, 10)\n",
      "step= 163 batchx_y = (128, 784) (128, 10)\n",
      "step= 164 batchx_y = (128, 784) (128, 10)\n",
      "step= 165 batchx_y = (128, 784) (128, 10)\n",
      "step= 166 batchx_y = (128, 784) (128, 10)\n",
      "step= 167 batchx_y = (128, 784) (128, 10)\n",
      "step= 168 batchx_y = (128, 784) (128, 10)\n",
      "step= 169 batchx_y = (128, 784) (128, 10)\n",
      "step= 170 batchx_y = (128, 784) (128, 10)\n",
      "Step 170, Minibatch Loss= 1607.1323, Training Accuracy= 0.547\n",
      "step= 171 batchx_y = (128, 784) (128, 10)\n",
      "step= 172 batchx_y = (128, 784) (128, 10)\n",
      "step= 173 batchx_y = (128, 784) (128, 10)\n",
      "step= 174 batchx_y = (128, 784) (128, 10)\n",
      "step= 175 batchx_y = (128, 784) (128, 10)\n",
      "step= 176 batchx_y = (128, 784) (128, 10)\n",
      "step= 177 batchx_y = (128, 784) (128, 10)\n",
      "step= 178 batchx_y = (128, 784) (128, 10)\n",
      "step= 179 batchx_y = (128, 784) (128, 10)\n",
      "step= 180 batchx_y = (128, 784) (128, 10)\n",
      "Step 180, Minibatch Loss= 1394.7042, Training Accuracy= 0.641\n",
      "step= 181 batchx_y = (128, 784) (128, 10)\n",
      "step= 182 batchx_y = (128, 784) (128, 10)\n",
      "step= 183 batchx_y = (128, 784) (128, 10)\n",
      "step= 184 batchx_y = (128, 784) (128, 10)\n",
      "step= 185 batchx_y = (128, 784) (128, 10)\n",
      "step= 186 batchx_y = (128, 784) (128, 10)\n",
      "step= 187 batchx_y = (128, 784) (128, 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step= 188 batchx_y = (128, 784) (128, 10)\n",
      "step= 189 batchx_y = (128, 784) (128, 10)\n",
      "step= 190 batchx_y = (128, 784) (128, 10)\n",
      "Step 190, Minibatch Loss= 1222.2650, Training Accuracy= 0.641\n",
      "step= 191 batchx_y = (128, 784) (128, 10)\n",
      "step= 192 batchx_y = (128, 784) (128, 10)\n",
      "step= 193 batchx_y = (128, 784) (128, 10)\n",
      "step= 194 batchx_y = (128, 784) (128, 10)\n",
      "step= 195 batchx_y = (128, 784) (128, 10)\n",
      "step= 196 batchx_y = (128, 784) (128, 10)\n",
      "step= 197 batchx_y = (128, 784) (128, 10)\n",
      "step= 198 batchx_y = (128, 784) (128, 10)\n",
      "step= 199 batchx_y = (128, 784) (128, 10)\n",
      "step= 200 batchx_y = (128, 784) (128, 10)\n",
      "Step 200, Minibatch Loss= 1149.8925, Training Accuracy= 0.594\n",
      "step= 201 batchx_y = (128, 784) (128, 10)\n",
      "step= 202 batchx_y = (128, 784) (128, 10)\n",
      "step= 203 batchx_y = (128, 784) (128, 10)\n",
      "step= 204 batchx_y = (128, 784) (128, 10)\n",
      "step= 205 batchx_y = (128, 784) (128, 10)\n",
      "step= 206 batchx_y = (128, 784) (128, 10)\n",
      "step= 207 batchx_y = (128, 784) (128, 10)\n",
      "step= 208 batchx_y = (128, 784) (128, 10)\n",
      "step= 209 batchx_y = (128, 784) (128, 10)\n",
      "step= 210 batchx_y = (128, 784) (128, 10)\n",
      "Step 210, Minibatch Loss= 1330.5364, Training Accuracy= 0.578\n",
      "step= 211 batchx_y = (128, 784) (128, 10)\n",
      "step= 212 batchx_y = (128, 784) (128, 10)\n",
      "step= 213 batchx_y = (128, 784) (128, 10)\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (256, 10) for Tensor u'Placeholder_43:0', which has shape '(?, 25)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-2d002097b08e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     print(\"Testing Accuracy:\",         sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n\u001b[1;32m     33\u001b[0m                                       \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                       keep_prob: 1.0}))\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (256, 10) for Tensor u'Placeholder_43:0', which has shape '(?, 25)'"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    idxminibathches = np.random.permutation(NB) # shuffling \n",
    "    for step in range(1, num_steps):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        print('step=',step,'batchx_y =', batch_x.shape, batch_y.shape)\n",
    "        ########################\n",
    "        i = idxminibathches[step] # index of minibatch\n",
    "        \n",
    "        # extract i-th minibatch from xtrain and ltrain \n",
    "        idxsmp = np.arange(i*B,np.min((i*B+B,N)))          # indicies of samples for i-th minibatch\n",
    "        \n",
    "        batch_x = xtrain[idxsmp]\n",
    "        batch_y = ltrain[idxsmp]\n",
    "        #################\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "N = xtrain.shape[0]            # training set size \n",
    "B = batch_size                 # minibatch size\n",
    "NB = N / B                     # number of minibatches\n",
    "NB = np.round(NB).astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "500\n",
      "(214,)\n",
      "27455\n",
      "128\n",
      "214\n",
      "214\n"
     ]
    }
   ],
   "source": [
    "print(step)\n",
    "print(num_steps)\n",
    "print(idxminibathches.shape)\n",
    "print(N)\n",
    "print(B)\n",
    "print(NB)\n",
    "print(step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 214 is out of bounds for axis 0 with size 214",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-44fb956fed03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxminibathches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# index of minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# extract i-th minibatch from xtrain and ltrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0midxsmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# indicies of samples for i-th minibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 214 is out of bounds for axis 0 with size 214"
     ]
    }
   ],
   "source": [
    "########################\n",
    "i = idxminibathches[step] # index of minibatch\n",
    "\n",
    "# extract i-th minibatch from xtrain and ltrain \n",
    "idxsmp = np.arange(i*B,np.min((i*B+B,N)))          # indicies of samples for i-th minibatch\n",
    "\n",
    "batch_x = xtrain[idxsmp]\n",
    "batch_y = ltrain[idxsmp]\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
