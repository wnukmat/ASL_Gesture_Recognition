{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hand Gesture Recogination Convolutional Neural Network\n",
    "  \n",
    "Author:       Mansur Amin <br>\n",
    "Team:         ASL Dynamics <br>\n",
    "Team Members: Matt Wnuk, Juan Castillo <br>\n",
    "Class:        ECE 285 Spring 2018 : Machine Learning for Computer Vision <br>\n",
    "\n",
    "- Build a convolutional neural network with TensorFlow for for gesture recogination using the MNIST ASL Dataset avaiable on Kaggel.\n",
    "\n",
    "- This project uses TensorFlow layers API for a raw TensorFlow implementation with variables. <br>\n",
    "Reference: Aymeric Damien's project to recognize digits in images from the MNIST digits dataset <br>\n",
    "Reference source: https://github.com/aymericdamien/TensorFlow-Examples/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Overview\n",
    "\n",
    "Convolutional Neural network \n",
    "\n",
    "![CNN](https://cdn-images-1.medium.com/max/1400/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg)\n",
    "\n",
    "## MNIST Dataset Overview\n",
    "\n",
    "This example is using MNIST ASL Hand Gestures. The dataset contains 27,456 examples for training and 7,173 examples for testing. The images have been size-normalized and centered in a fixed-size image (28x28 pixels) with values from 0 to 1. For simplicity, each image has been flattened and converted to a 1-D numpy array of 784 features (28*28).\n",
    "\n",
    "![MNIST](https://www.kaggle.com/databundle/preview/image-dataset/3258/5337/5337/15172)\n",
    "\n",
    "More info: https://www.kaggle.com/datamunge/sign-language-mnist/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "####################################################################\n",
    "# Libraries \n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "#####################################################################\n",
    "# def read_data(path) :\n",
    "# Reads data specidfed by path and stores into numpy array, Splits data nad lbl\n",
    "# path = location of where data is stored\n",
    "#####################################################################\n",
    "def read_data(path) :\n",
    "    num_lines = sum(1 for line in open(path)) # get number of lines in file\n",
    "    myfile = open(path,'r')\n",
    "    j = 0\n",
    "    read_lines = np.zeros((num_lines,785))    # expects a 25x25 pixel row vector\n",
    "    for line in myfile :\n",
    "        if(j >= 1) :\n",
    "            read_lines[j,:] = np.asarray(line.split(','))\n",
    "        j = j+1\n",
    "    myfile.close()\n",
    "    \n",
    "    print('Initial', path[49:54], 'Shape:', read_lines.shape)\n",
    "    read_lines = np.delete(read_lines,0,0) # remove pixel labels row from frost row\n",
    "    labels = read_lines[:,0]\n",
    "    read_lines = np.delete(read_lines,0,1) # remove labels from first collumn\n",
    "    print('Augmented', path[49:54], 'Shape:', read_lines.shape)\n",
    "    return read_lines,labels.astype(np.int16) # .reshape(len(labels),1)\n",
    "\n",
    "#####################################################################\n",
    "# def vis_data(dat_vec,dat_lbl) :\n",
    "# Visualize data.\n",
    "# dat_vec = vector representing data\n",
    "# dat_lbl = label representing data\n",
    "#####################################################################\n",
    "def vis_data(dat_vec,dat_lbl) :\n",
    "    lbl_map = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "    dat_mat = dat_vec.reshape(28,28)\n",
    "    plt.title('Image Label = ' + str(dat_lbl) + ' = ' + lbl_map[dat_lbl])\n",
    "    plt.imshow(dat_mat)\n",
    "    \n",
    "#####################################################################\n",
    "# def normalize_MNIST_images(x) :\n",
    "# Normalize data in range of -1 to 1\n",
    "# x = vector representing data\n",
    "#####################################################################\n",
    "def normalize_MNIST_images(x,max_x, diff) : \n",
    "    x = x.astype(np.float64)\n",
    "    x = max_x*(x-np.min(x))/(np.max(x)-np.min(x))-diff\n",
    "    print(\"Normalize_MNIST_images\", x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train Shape: (27456, 785)\n",
      "Augmented train Shape: (27455, 784)\n",
      "Initial test. Shape: (7173, 785)\n",
      "Augmented test. Shape: (7172, 784)\n",
      "Normalize_MNIST_images (27455, 784)\n",
      "Normalize_MNIST_images (7172, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read ASL MNIST data\n",
    "####################################################################\n",
    "# # Windows Path\n",
    "# train_path = 'C:\\\\Users\\\\maamin\\\\Desktop\\\\ECE_285_ML_DL\\\\data\\\\sign_mnist_train.csv'\n",
    "# test_path = 'C:\\\\Users\\\\maamin\\\\Desktop\\\\ECE_285_ML_DL\\\\data\\\\sign_mnist_test.csv'\n",
    "####################################################################\n",
    "# # Mac path\n",
    "# train_path = '/Users/mansuramin/Desktop/ECE_285_ML/data/sign_mnist_train.csv'\n",
    "# test_path = '/Users/mansuramin/Desktop/ECE_285_ML/data/sign_mnist_test.csv'\n",
    "####################################################################\n",
    "# Linux Server Path\n",
    "train_path = '/datasets/home/56/256/maamin/asl_data/sign_mnist_train.csv'\n",
    "test_path = '/datasets/home/56/256/maamin/asl_data/sign_mnist_test.csv'\n",
    "####################################################################\n",
    "# Load data from path\n",
    "xtrain,ltrain = read_data(train_path)\n",
    "xtest,ltest = read_data(test_path)\n",
    "####################################################################\n",
    "# Normalize between 0 and 1\n",
    "xtrain = normalize_MNIST_images(xtrain,1,0)\n",
    "xtest = normalize_MNIST_images(xtest,1,0)\n",
    "####################################################################\n",
    "# Change Types to match nn types\n",
    "xtrain = xtrain.astype(np.float32)\n",
    "ltrain = ltrain.astype(np.uint8)\n",
    "xtest = xtest.astype(np.float32)\n",
    "ltest = ltest.astype(np.uint8)\n",
    "####################################################################\n",
    "# Visualize an image as test\n",
    "indx = 10240\n",
    "\n",
    "vis_data(xtrain[indx,:],ltrain[indx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 72407.8906, Training Accuracy= 0.109\n",
      "Step 10, Minibatch Loss= 27264.1016, Training Accuracy= 0.242\n",
      "Step 20, Minibatch Loss= 10460.0596, Training Accuracy= 0.539\n",
      "Step 30, Minibatch Loss= 6482.0068, Training Accuracy= 0.703\n",
      "Step 40, Minibatch Loss= 3905.6238, Training Accuracy= 0.812\n",
      "Step 50, Minibatch Loss= 2389.6243, Training Accuracy= 0.906\n",
      "Step 60, Minibatch Loss= 3640.3276, Training Accuracy= 0.812\n",
      "Step 70, Minibatch Loss= 3553.6602, Training Accuracy= 0.859\n",
      "Step 80, Minibatch Loss= 3361.6553, Training Accuracy= 0.883\n",
      "Step 90, Minibatch Loss= 1805.4478, Training Accuracy= 0.883\n",
      "Step 100, Minibatch Loss= 2948.9309, Training Accuracy= 0.836\n",
      "Step 110, Minibatch Loss= 2379.0520, Training Accuracy= 0.906\n",
      "Step 120, Minibatch Loss= 3342.1931, Training Accuracy= 0.852\n",
      "Step 130, Minibatch Loss= 3353.4324, Training Accuracy= 0.875\n",
      "Step 140, Minibatch Loss= 1690.9668, Training Accuracy= 0.914\n",
      "Step 150, Minibatch Loss= 895.6429, Training Accuracy= 0.953\n",
      "Step 160, Minibatch Loss= 1044.5953, Training Accuracy= 0.930\n",
      "Step 170, Minibatch Loss= 1025.2166, Training Accuracy= 0.930\n",
      "Step 180, Minibatch Loss= 1069.3907, Training Accuracy= 0.922\n",
      "Step 190, Minibatch Loss= 1594.9666, Training Accuracy= 0.891\n",
      "Step 200, Minibatch Loss= 1576.8420, Training Accuracy= 0.914\n",
      "Step 210, Minibatch Loss= 779.8536, Training Accuracy= 0.930\n",
      "Step 220, Minibatch Loss= 1499.9558, Training Accuracy= 0.930\n",
      "Step 230, Minibatch Loss= 2154.7717, Training Accuracy= 0.891\n",
      "Step 240, Minibatch Loss= 960.6990, Training Accuracy= 0.938\n",
      "Step 250, Minibatch Loss= 936.6736, Training Accuracy= 0.945\n",
      "Step 260, Minibatch Loss= 1144.2389, Training Accuracy= 0.891\n",
      "Step 270, Minibatch Loss= 1076.5941, Training Accuracy= 0.945\n",
      "Step 280, Minibatch Loss= 614.1275, Training Accuracy= 0.945\n",
      "Step 290, Minibatch Loss= 1169.7988, Training Accuracy= 0.914\n",
      "Step 300, Minibatch Loss= 1029.7633, Training Accuracy= 0.930\n",
      "Step 310, Minibatch Loss= 1267.6233, Training Accuracy= 0.891\n",
      "Step 320, Minibatch Loss= 1477.2605, Training Accuracy= 0.922\n",
      "Step 330, Minibatch Loss= 985.5542, Training Accuracy= 0.914\n",
      "Step 340, Minibatch Loss= 836.8495, Training Accuracy= 0.930\n",
      "Step 350, Minibatch Loss= 679.9211, Training Accuracy= 0.969\n",
      "Step 360, Minibatch Loss= 622.1238, Training Accuracy= 0.953\n",
      "Step 370, Minibatch Loss= 217.9361, Training Accuracy= 0.969\n",
      "Step 380, Minibatch Loss= 686.3157, Training Accuracy= 0.930\n",
      "Step 390, Minibatch Loss= 810.8251, Training Accuracy= 0.938\n",
      "Step 400, Minibatch Loss= 346.2815, Training Accuracy= 0.945\n",
      "Step 410, Minibatch Loss= 588.3010, Training Accuracy= 0.945\n",
      "Step 420, Minibatch Loss= 706.2130, Training Accuracy= 0.953\n",
      "Step 430, Minibatch Loss= 1202.9146, Training Accuracy= 0.914\n",
      "Step 440, Minibatch Loss= 679.0670, Training Accuracy= 0.953\n",
      "Step 450, Minibatch Loss= 539.9678, Training Accuracy= 0.953\n",
      "Step 460, Minibatch Loss= 447.8742, Training Accuracy= 0.945\n",
      "Step 470, Minibatch Loss= 217.2316, Training Accuracy= 0.977\n",
      "Step 480, Minibatch Loss= 264.3403, Training Accuracy= 0.977\n",
      "Step 490, Minibatch Loss= 256.5056, Training Accuracy= 0.984\n",
      "Step 500, Minibatch Loss= 405.7829, Training Accuracy= 0.969\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9765625\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "#####################################################################\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "#####################################################################\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = mnist.train.next_batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 784)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(NB).astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "N = xtrain.shape[0]            # training set size \n",
    "B = batch_size                  # minibatch size\n",
    "NB = N / B                      # number of minibatches\n",
    "NB = np.round(NB).astype(np.long)\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 25 # MNIST total classes (0-25 gestures)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (128,) for Tensor u'Placeholder_16:0', which has shape '(?, 25)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e623eac89a12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1097\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (128,) for Tensor u'Placeholder_16:0', which has shape '(?, 25)'"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    idxminibathches = np.random.permutation(NB) # shuffling \n",
    "    for step in range(1, num_steps+1):\n",
    "#         batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        ########################\n",
    "        i = idxminibathches[step] # index of minibatch\n",
    "        \n",
    "        # extract i-th minibatch from xtrain and ltrain \n",
    "        idxsmp = np.arange(i*B,np.min((i*B+B,N)))          # indicies of samples for i-th minibatch\n",
    "        \n",
    "        batch_x = xtrain[idxsmp]\n",
    "        batch_y = ltrain[idxsmp]\n",
    "        #################\n",
    "        \n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
